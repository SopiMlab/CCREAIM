# @package _global_
process:
  train: true
  cross_val_k: 0

logging:
  exp_name: 8bit_system
  checkpoint: 1
  save_pred: false
  save_one_per_batch: false

data:
  data_tar: ???

hyper:
  model: transformer
  latent_dim: 256
  seed: 0
  seq_len: 1600
  epochs: 30
  batch_size: 32
  learning_rate: 1e-4
  transformer:
    num_heads_latent_dimension_div: 64
    num_enc_layers: 3
    num_dec_layers: 3
    autoregressive_loss_weight: 1
    linear_map: true
  vqvae:
    num_embeddings: 256
    beta: 0
    reset_patience: -1
